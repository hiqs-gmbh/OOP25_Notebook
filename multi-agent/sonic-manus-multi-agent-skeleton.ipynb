{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agenten-System mit LangGraph (Whisper, NeMo, Matching & Themenextraktion)\n",
    "\n",
    "In diesem Workshop bauen wir ein **Multi-Agenten-System** mithilfe von **LangGraph**. Folgende Agenten (Nodes) kommen zum Einsatz:\n",
    "\n",
    "1. **Whisper-Agent**: Führt Speech-to-Text mit Whisper durch.\n",
    "2. **Pyannote-Agent**: Führt die Sprechertrennung durch.\n",
    "3. **Matching-Agent**: Verknüpft die Ergebnisse von Whisper und Pyannote.\n",
    "4. **Summarization-Agent**: Fasst den gefundenen Dialog/Gesprächsinhalte zusammen.\n",
    "5. **Topic-Agent**: Extrahiert ein **Thema** oder Kern-Aussage aus dem Gespräch.\n",
    "6. **Search-Agent**: Frag das Internet nach diesem Thema (z. B. via Tavily) und holt relevante Informationen.\n",
    "\n",
    "**Ziel**: Am Ende haben wir einen Workflow, bei dem eine Audiodatei in ein Protokoll gewandelt wird, wir eine **Zusammenfassung** erhalten, ein **Thema** extrahieren und dazu eine **Internetrecherche** durchführen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from tavily import TavilyClient\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from foundation.helper import run_whisper, run_pyannote, run_matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Laden der Umgebungsvariablen und erstellen der Chat Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PYANNOTE_HF_TOKEN = os.getenv(\"PYANNOTE_HF_TOKEN\", \"\")\n",
    "PYANNOTE_MODEL = os.getenv(\"PYANNOTE_MODEL\", \"PYANNOTE_MODEL\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://host.docker.internal:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:1b\")\n",
    "STACKIT_VLLM_API_KEY = os.getenv(\"STACKIT_VLLM_API_KEY\", \"\")\n",
    "STACKIT_MODEL = os.getenv(\"STACKIT_MODEL\", \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\")\n",
    "STACKIT_BASE_URL = os.getenv(\"STACKIT_BASE_URL\", \"https://api.openai-compat.model-serving.eu01.onstackit.cloud/v1\")\n",
    "\n",
    "\n",
    "stackit_llm = ChatOpenAI(\n",
    "    model=STACKIT_MODEL,\n",
    "    base_url=STACKIT_BASE_URL,\n",
    "    api_key=STACKIT_VLLM_API_KEY,\n",
    ")\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    model=OLLAMA_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    temperature=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. State definieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyState(TypedDict):\n",
    "    audio_path: str\n",
    "    # ...\n",
    "    internet_search_result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Node definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def whisper(state: MyState):\n",
    "    result = run_whisper(audio_path=state[\"audio_path\"])\n",
    "    return {\"whisper_result\": result}\n",
    "\n",
    "# Node\n",
    "def pyannote(state: MyState):\n",
    "    pass\n",
    "\n",
    "# Node\n",
    "def matcher(state: MyState):\n",
    "   pass\n",
    "\n",
    "# Node\n",
    "def summarize(state: MyState):\n",
    "    messages_for_llm = [\n",
    "        ]\n",
    "    pass\n",
    "\n",
    "# Node\n",
    "def extract_topic(state: MyState):\n",
    "    messages_for_llm = [\n",
    "        AIMessage(content=\"You are a helpful assistant and an expert in extracting the main topic of the conversation out of a summary.\"),\n",
    "        HumanMessage(content=f\"Please extract one single topic out of this summary: {state[\"summary\"]}\")\n",
    "    ]\n",
    "\n",
    "    topic = stackit_llm.invoke(messages_for_llm)\n",
    "    return {\"topic\": topic.content}\n",
    "\n",
    "# Node\n",
    "def internet_search(state: MyState):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Graph definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StateGraph initialisieren\n",
    "graph = StateGraph(MyState)\n",
    "\n",
    "# Agent (Node) hinzufügen\n",
    "graph.add_node(\"Whisper\", whisper)\n",
    "# ...\n",
    "graph.add_node(\"Internet Researcher\", internet_search)\n",
    "\n",
    "# Ablauf im Graph definieren durch Edges\n",
    "graph.add_edge(START, \"Whisper\")\n",
    "# ...\n",
    "graph.add_edge(\"Internet Researcher\", END)\n",
    "\n",
    "# Graph erstellen\n",
    "graph = graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Graph visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph anzeigen lassen\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Graph aufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph aufrufen\n",
    "audio_path = \"./foundation/audios/Sonic-Manus-Pitch.mp3\"\n",
    "\n",
    "result_state = graph.invoke({\"audio_path\": audio_path}, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ergebnisse ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
