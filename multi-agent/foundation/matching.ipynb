{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenführen von Whisper-Transkript und Pyannote-Diarization (Matching)\n",
    "\n",
    "In diesem Workshop lernst du, wie man das **transkribierte** Ergebnis von **Whisper** (Chunks mit Zeitstempeln und Text) mit dem Ergebnis der **Pyannote Speaker Diarization** (Sprecher und deren Zeitintervalle) kombiniert. Ziel ist es, jedem Text-Chunk einen Sprecher zuzuweisen.\n",
    "\n",
    "Voraussetzungen:\n",
    "- Eine Datei mit dem **Whisper**-Output, z. B. `whisper_data.json`, die zumindest folgende Struktur hat:\n",
    "  ```json\n",
    "  {\n",
    "    \"text\": \"...\",\n",
    "    \"chunks\": [\n",
    "      {\n",
    "        \"timestamp\": [ start, end ],\n",
    "        \"text\": \"...\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- Eine Datei mit den Pyannote-Ergebnissen, z. B. diarization_data.json, welche ein Array von Segmenten enthält:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"start\": 0.0,\n",
    "    \"end\": 1.5,\n",
    "    \"speaker\": \"SPEAKER_00\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dateipfade spezifizieren\n",
    "- `whisper_folder` verweist auf den Ordner mit den Whisper-Ergebnissen.\n",
    "- `pyannote_folder` verweist auf den Ordner mit den Pyannote-Ergebnissen.\n",
    "- `matched_transcripts_folder` ist der Zielordner, in dem wir die **kombinierten** Daten ablegen.\n",
    "- `audio_file_name` ist der Basename der entsprechenden Audiodatei (z. B. `\"Sonic-Manus-Pitch\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spezifierung von Verzeichnissen und Dateinamen\n",
    "whisper_folder = \"./outputs/whisper/\"\n",
    "pyannote_folder = \"./outputs/pyannote/\"\n",
    "matched_transcripts_folder = \"./outputs/matched_transcripts/\"\n",
    "audio_file_name = \"Sonic-Manus-Pitch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Einlesen der JSON-Dateien\n",
    "Wir lesen:\n",
    "1. Die Whisper-Ergebnisse (`whisper_data.json`) mit den Chunks.\n",
    "2. Die Pyannote-Ergebnisse (`diarization_data.json`) mit den Sprechersegmenten.\n",
    "\n",
    "Anschließend liegen uns zwei Python-Objekte vor:\n",
    "- `whisper_data` mit u. a. `whisper_data[\"chunks\"]`\n",
    "- `diarization_data` als Liste von Dictionaries mit `[{\"start\":..., \"end\":..., \"speaker\":...}, ...]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dateien einlesen\n",
    "with open(f\"{whisper_folder}{audio_file_name}.json\", \"r\") as whisper_file:\n",
    "    whisper_data = json.load(whisper_file)\n",
    "\n",
    "with open(f\"{pyannote_folder}{audio_file_name}.json\", \"r\") as diarization_file:\n",
    "    diarization_data = json.load(diarization_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Zusammenführen der Chunks\n",
    "\n",
    "Für jeden **Whisper-Chunk** (mit `start`, `end` und `text`) suchen wir den passenden **Sprecher** in den Diarization-Daten. \n",
    "\n",
    "**Logik**:\n",
    "- Falls das Zeitintervall `[chunk_start, chunk_end]` vollständig in `[segment.start, segment.end]` liegt, ordnen wir diesen Sprecher zu.\n",
    "- Falls es eine Überlappung gibt (nicht 100%ig passend, aber Start oder Ende überschneiden sich), weisen wir auch diesen Sprecher zu. \n",
    "\n",
    "Wenn kein Sprecher-Segment gefunden wird, setzen wir `speaker = \"Unknown\"` und geben eine Warnung aus.\n",
    "\n",
    "\n",
    "Am Ende erstellen wir ein Array `merged_output`, in dem jeder Eintrag so aufgebaut ist:\n",
    "```json\n",
    "{\n",
    "  \"speaker\": \"SPEAKER_XX\",\n",
    "  \"text\": \"...\",\n",
    "  \"start\": 0.0,\n",
    "  \"end\": 1.2\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste für die zusammengeführten Ergebnisse\n",
    "merged_output = []\n",
    "\n",
    "# Durch alle Whisper-Chunks iterieren\n",
    "for chunk in whisper_data[\"chunks\"]:\n",
    "    chunk_start = chunk[\"timestamp\"][0]\n",
    "    chunk_end = chunk[\"timestamp\"][1]\n",
    "    chunk_text = chunk[\"text\"]\n",
    "\n",
    "    # Standard-Sprecher = \"Unknown\"\n",
    "    speaker = \"Unknown\"\n",
    "\n",
    "    # Überprüfe, ob dieses Chunk mit einem Diarization-Segment übereinstimmt\n",
    "    for segment in diarization_data:\n",
    "        if chunk_start >= segment[\"start\"] and chunk_end <= segment[\"end\"]:\n",
    "            # Das Whisper-Chunk liegt komplett in diesem Segment\n",
    "            speaker = segment[\"speaker\"]\n",
    "            break\n",
    "        elif chunk_start <= segment[\"end\"] and chunk_end >= segment[\"start\"]:\n",
    "            # Teil-Überlappung\n",
    "            speaker = segment[\"speaker\"]\n",
    "            break\n",
    "\n",
    "    if speaker == \"Unknown\":\n",
    "        print(f\"Warning: No speaker found for chunk '{chunk_text}' [{chunk_start:.2f}s to {chunk_end:.2f}s]\")\n",
    "\n",
    "    merged_output.append({\n",
    "        \"speaker\": speaker,\n",
    "        \"text\": chunk_text.strip(),\n",
    "        \"start\": chunk_start,\n",
    "        \"end\": chunk_end\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ausgabe von Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Moin, was ist denn das Sonic Manus Projekt?\",\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 3.2\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Das Sonic Manus Projekt ist ein kairosiertes Transcription Tool.\",\n",
      "        \"start\": 3.2,\n",
      "        \"end\": 7.04\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"Das Tolle daran ist, dass es auch die unterschiedlichen Sprecher in einem Gespr\\u00e4ch aufgeteilt bekommt.\",\n",
      "        \"start\": 7.04,\n",
      "        \"end\": 13.0\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Okay, aber das kann Teams doch l\\u00e4ngst.\",\n",
      "        \"start\": 13.0,\n",
      "        \"end\": 15.88\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Er schon, aber Teams k\\u00f6nnen daf\\u00fcr einfach die unterschiedlichen Ger\\u00e4te von den Benutzern verwenden\",\n",
      "        \"start\": 15.88,\n",
      "        \"end\": 21.0\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"und wir k\\u00f6nnen das ganz von alleine.\",\n",
      "        \"start\": 21.0,\n",
      "        \"end\": 22.6\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"Genau, wir brauchen nur das Audio und zus\\u00e4tzlich l\\u00e4uft es bei uns lokal.\",\n",
      "        \"start\": 22.6,\n",
      "        \"end\": 27.6\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Mika will ich haben.\",\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 2.0\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(merged_output, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Datei Speichern\n",
    "\n",
    "- Nun speichern wir das Ergebnis in einer `.json` in den Zielordner (`matched_transcripts_folder`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Datei speichern\n",
    "with open(f\"{matched_transcripts_folder}{audio_file_name}.json\", \"w\") as output_file:\n",
    "    json.dump(merged_output, output_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Schritt haben wir:\n",
    "1. **Whisper**-Transkripte (Text + Zeitstempel) eingelesen.\n",
    "2. **Pyannote**-Diarization-Segmente (Sprecher + Zeitintervalle) eingelesen.\n",
    "3. Für jedes Whisper-Chunk basierend auf dem Zeitintervall einen **Sprecher** ermittelt.\n",
    "4. Das Ergebnis in einer JSON-Datei gespeichert.\n",
    "\n",
    "Damit hast du eine **Sprecher-annotierte Transkription**, in der jeder Abschnitt weiß, von wem er gesprochen wurde. Diese Daten können z. B. für **Interviews**, **Meetings** oder **Podcasts** sehr hilfreich sein, um mehr Informationen als nur Text zu haben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
