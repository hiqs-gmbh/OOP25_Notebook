{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text mit Hugging Face Transformers (Whisper)\n",
    "\n",
    "In diesem Workshop lernst du, wie du mithilfe von **Hugging Face**-Modellen (speziell Whisper) Sprachaufnahmen automatisiert transkribieren kannst. Wir verwenden dabei das Modell [openai/whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo).\n",
    "\n",
    "Voraussetzungen:\n",
    "- Installierte Pakete:\n",
    "  - `transformers`\n",
    "  - `torchaudio`\n",
    "  - `tqdm`\n",
    "  - `huggingface_hub`\n",
    "- Eine Audio-Datei (z. B. `.mp3`, `.wav`) zum Testen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dateipfade spezifizieren\n",
    "- Wir definieren Ordner und Dateinamen (z. B. `audio_file_folder`, `audio_name`, `audio_format`, `whisper_folder`).\n",
    "- Lege in `audio_file_folder` deine eigenen Audiodateien ab und ändere `audio_name` entsprechend, um deine eigenen Dateien zu transkribieren\n",
    "- Damit wir die Ausgabe von Whisper nachher weiterverwenden können, speichern wir die Ausgabe später in einer .json File im `whisper_folder` ab. \n",
    "\n",
    "Wir benötigen eine **mp3** Datei als Eingabe für Whisper. Mit **ffmpeg** können wir Audiodateien von anderem Format konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i ./audios/<audio_name>.m4a ./audios/<audio_name>.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spezifierung von Verzeichnissen und Dateinamen\n",
    "audio_file_folder = \"./audios/\"\n",
    "audio_name = \"Michi_Sonic_Manus\"\n",
    "audio_format = \".mp3\"\n",
    "whisper_folder = \"./outputs/whisper/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Device wählen (GPU oder CPU)\n",
    "Wir wählen automatisch die GPU aus, wenn `torch.cuda.is_available()` den Wert `True` zurückgibt. Ansonsten fallen wir auf die CPU zurück. Zudem passen wir den **Datentyp** (`torch_dtype`) an, um auf der GPU mit `float16` rechnen zu können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Laden des Modells und Prozessors\n",
    "\n",
    "- **AutoModelForSpeechSeq2Seq**: Lädt ein Modell, das speziell für die Sprach-zu-Text-Aufgaben (Speech-to-Text) ausgelegt ist.\n",
    "- **AutoProcessor**: Lädt den passenden Tokenizer und Audio-Feature-Extractor, um die Audiodatei in das richtige Input-Format für das Modell zu konvertieren.\n",
    "\n",
    "Anschließend verschieben wir das Modell auf das gewählte `device`.\n",
    "\n",
    "Das [Modell wird von Huggingface](https://huggingface.co/openai/whisper-large-v3-turbo) heruntergeladen. Dort findest du auch weitere interessante Informationen über das Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell-ID von Whisper\n",
    "# model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model_id = \"openai/whisper-base\"\n",
    "\n",
    "# Modell laden\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Prozessor laden\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Festlegen der Sprache\n",
    "\n",
    "Mit `forced_decoder_ids` legen wir fest, dass das Modell **Deutsch** transkribieren soll. \n",
    "Der Aufruf `processor.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")` erzeugt die IDs, die sicherstellen, dass das Modell deutschsprachigen Text als Output generiert.\n",
    "\n",
    "Probiere gerne andere Sprachen aus und schaue wie das funktioniert passiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forced Decoder IDs für Deutsch (Transkription)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline erstellen\n",
    "\n",
    "Wir erstellen eine Pipeline vom Typ `\"automatic-speech-recognition\"`. Damit:\n",
    "- wird das Modell und der Tokenizer verknüpft\n",
    "- das Audio-Feature-Extrahieren übernimmt `feature_extractor`\n",
    "- im `generate_kwargs` können wir die `forced_decoder_ids` übergeben\n",
    "- mit `return_timestamps=True` erhalten wir ggf. Wort- oder Chunk-basierte Zeitstempel\n",
    "\n",
    "So können wir direkt eine Audiodatei an `pipe(...)` übergeben, ohne manuell Tokenizing etc. durchzuführen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline einrichten\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Transkribieren der Datei\n",
    "- Das Modell transrkribiert die Datei\n",
    "- `pipe(...)` gibt ein Dictionary mit Feldern zurück (z. B. `\"text\"` für die transkribierte Ausgabe, `\"chunks\"` für die Zeitstempel etc.).\n",
    "- Mit `json.dumps(result, indent=4)` können wir das gesamte Dictionary (mit Timestamps, Token-Infos etc.) schön formatiert ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"text\": \" H\\u00e4tte ich noch nicht auf der HHTS Webseite und habe da dem Blog bei Track zu Sonic Manus gesehen und es hat mich sehr neugierig gemacht. Kannst du das f\\u00fcr meine Zuschauer kurz eine 2-3 Setze beschreiben? Ja klar kann ich. Sonic Manus kannst du ganz einfach Meetings erstellen und dann Audi Datein hochladen und die werden dann transkippiert. Was genau meinst du mit transkippiert? Na ja da wird dann festgelegt, wer was gesagt hat und was gesagt wurde. Das hei\\u00dft, wenn mir jetzt hier soll ich mal uns laufen lassen w\\u00fcrden, dann h\\u00e4tt ich danach ne Datei oder ne, also die Audio Datei von dem, was wir gesagt haben und getrennt bei was gesagt hat. Wichtig. Und dann als Audio oder wie werden die Daten bei der Frau beidet? Ja richtig. Wir haben dann das gesprochen, ne Wort und aufgeteilt eben, welcher Sprecher was gesagt hat. Ich kann dann auch dein Name zum Beispiel eintragen und ist tolle dann ist, der wird automatisch noch eine Zusammenfassung generiert und in der Zusammenfassung sind verschiedene Stichpunkte enthalten welche Themen da abgesprochen wurden genau cool\",\n",
      "    \"chunks\": [\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                0.0,\n",
      "                6.56\n",
      "            ],\n",
      "            \"text\": \" H\\u00e4tte ich noch nicht auf der HHTS Webseite und habe da dem Blog bei Track zu Sonic Manus gesehen\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                6.56,\n",
      "                8.36\n",
      "            ],\n",
      "            \"text\": \" und es hat mich sehr neugierig gemacht.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                8.36,\n",
      "                12.16\n",
      "            ],\n",
      "            \"text\": \" Kannst du das f\\u00fcr meine Zuschauer kurz eine 2-3 Setze beschreiben?\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                12.16,\n",
      "                13.64\n",
      "            ],\n",
      "            \"text\": \" Ja klar kann ich.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                13.64,\n",
      "                19.0\n",
      "            ],\n",
      "            \"text\": \" Sonic Manus kannst du ganz einfach Meetings erstellen und dann Audi Datein hochladen und die\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                19.0,\n",
      "                20.0\n",
      "            ],\n",
      "            \"text\": \" werden dann transkippiert.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                20.0,\n",
      "                22.64\n",
      "            ],\n",
      "            \"text\": \" Was genau meinst du mit transkippiert?\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                22.64,\n",
      "                27.28\n",
      "            ],\n",
      "            \"text\": \" Na ja da wird dann festgelegt, wer was gesagt hat und was gesagt wurde.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                0.0,\n",
      "                6.0\n",
      "            ],\n",
      "            \"text\": \" Das hei\\u00dft, wenn mir jetzt hier soll ich mal uns laufen lassen w\\u00fcrden, dann h\\u00e4tt ich danach\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                6.0,\n",
      "                12.32\n",
      "            ],\n",
      "            \"text\": \" ne Datei oder ne, also die Audio Datei von dem, was wir gesagt haben und getrennt\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                12.32,\n",
      "                13.32\n",
      "            ],\n",
      "            \"text\": \" bei was gesagt hat.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                13.32,\n",
      "                14.32\n",
      "            ],\n",
      "            \"text\": \" Wichtig.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                14.32,\n",
      "                17.32\n",
      "            ],\n",
      "            \"text\": \" Und dann als Audio oder wie werden die Daten bei der Frau beidet?\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                17.32,\n",
      "                18.32\n",
      "            ],\n",
      "            \"text\": \" Ja richtig.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                18.32,\n",
      "                23.44\n",
      "            ],\n",
      "            \"text\": \" Wir haben dann das gesprochen, ne Wort und aufgeteilt eben, welcher Sprecher was gesagt\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                23.44,\n",
      "                24.44\n",
      "            ],\n",
      "            \"text\": \" hat.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                24.44,\n",
      "                27.32\n",
      "            ],\n",
      "            \"text\": \" Ich kann dann auch dein Name zum Beispiel eintragen und ist tolle dann ist, der wird\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                0.0,\n",
      "                4.8\n",
      "            ],\n",
      "            \"text\": \" automatisch noch eine Zusammenfassung generiert und in der Zusammenfassung sind verschiedene Stichpunkte\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                4.8,\n",
      "                9.72\n",
      "            ],\n",
      "            \"text\": \" enthalten welche Themen da abgesprochen wurden genau cool\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = pipe(f\"{audio_file_folder}{audio_name}{audio_format}\", generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids})\n",
    "\n",
    "# Ergebnis als JSON ausgeben\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Datei Speichern\n",
    "- `result[\"text\"]` enthält den transkribierten Text.\n",
    "- Nun speichern wir das Ergebnis in einer `.json`-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Datei speichern\n",
    "with open(f\"{whisper_folder}{audio_name}.json\", \"w\") as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Workshop haben wir:\n",
    "1. Ein **Whisper-Modell** (openai/whisper-large-v3-turbo) geladen und konfiguriert.\n",
    "2. Eine **Pipeline** für die automatische Spracherkennung eingerichtet.\n",
    "3. **Forced Decoder IDs** benutzt, um Deutsch als Transkriptionssprache festzulegen.\n",
    "4. Eine **Audiodatei** transkribiert und das Ergebnis als Text und JSON ausgegeben.\n",
    "\n",
    "Damit hast du ein Grundgerüst, um eigene Audiodateien zu transkribieren oder Übersetzungen in andere Sprachen durchzuführen. \n",
    "- Passe je nach Bedarf die `forced_decoder_ids` an, um unterschiedliche Sprachen zu erzwingen.\n",
    "- Experimentiere mit verschiedenen Whisper- oder anderen Speech-to-Text-Modellen von Hugging Face.\n",
    "- Nutze die Timestamps für die Anzeige von Untertiteln oder Live-Transkriptionen.\n",
    "\n",
    "Viel Erfolg!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
