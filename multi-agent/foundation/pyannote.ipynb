{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Diarization mit Pyannote\n",
    "\n",
    "In diesem Workshop lernst du, wie man mithilfe der [pyannote.audio](https://github.com/pyannote/pyannote-audio) Bibliothek **Speaker Diarization** (Wer hat wann gesprochen?) durchführt.\n",
    "\n",
    "Wir verwenden:\n",
    "- **pyannote.audio** für das Pipeline-Modell\n",
    "\n",
    "Voraussetzungen:\n",
    "- Eine `.env`-Datei mit `PYANNOTE_HF_TOKEN` (dein Hugging Face Token) und `PYANNOTE_MODEL` (der Modellname, z. B. `\"pyannote/speaker-diarization\"` oder ähnliches).\n",
    "- Ein Audiodatei im `.wav`-Format (oder ein kompatibles Format) zum Testen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pyannote.audio import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Laden von Umgebungsvariablen\n",
    "\n",
    "- Wir greifen auf `PYANNOTE_HF_TOKEN` und `PYANNOTE_MODEL` zu, die in dieser Datei gesetzt sind.\n",
    "- `PYANNOTE_HF_TOKEN` ist dein persönlicher Hugging Face Authentication Token, notwendig zum Laden des Pyannote Modells.\n",
    "- `PYANNOTE_MODEL` ist die Pfadangabe des Modells, z. B. `\"pyannote/speaker-diarization\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Variablen aus .env (falls vorhanden)\n",
    "load_dotenv()\n",
    "\n",
    "PYANNOTE_HF_TOKEN = os.getenv(\"PYANNOTE_HF_TOKEN\", \"\")\n",
    "PYANNOTE_MODEL = os.getenv(\"PYANNOTE_MODEL\", \"PYANNOTE_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dateipfade spezifizieren\n",
    "- Wir definieren Ordner und Dateinamen (z. B. `audio_file_folder`, `audio_name`, `audio_format`, `pyannote_folder`).\n",
    "- Lege in `audio_file_folder` deine eigenen Audiodateien ab und ändere `audio_name` entsprechend, um deine eigenen Dateien zu transkribieren\n",
    "- Damit wir die Ausgabe von Pyannote nachher weiterverwenden können, speichern wir die Ausgabe später in einer .json File im `pyannote_folder` ab. \n",
    "\n",
    "Wir benötigen eine **.wav** Datei als Eingabe für Pyannote. Mit **ffmpeg** können wir Audiodateien von anderem Format konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: audio_name: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i ./audios/<audio_name>.m4a ./audios/<audio_name>.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spezifierung von Verzeichnissen und Dateinamen\n",
    "audio_file_folder = \"./audios/\"\n",
    "audio_name = \"Sonic-Manus-Pitch\"\n",
    "audio_format = \".wav\"\n",
    "pyannote_folder = \"./outputs/pyannote/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline laden und Audio verarbeiten\n",
    "\n",
    "- `Pipeline.from_pretrained(PYANNOTE_MODEL, use_auth_token=...)` lädt das gewählte Pyannote-Modell. \n",
    "- Anschließend rufen wir `pipeline(audio_file_path)` auf, um das Audio zu diarizieren.\n",
    "- Das Ergebnis (`diarization`) enthält die zeitlichen Segmente und die vermuteten Sprecher (Labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/usr/local/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    }
   ],
   "source": [
    "# Laden der Diarization-Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  PYANNOTE_MODEL,\n",
    "  use_auth_token=PYANNOTE_HF_TOKEN\n",
    ")\n",
    "\n",
    "# Audio verarbeiten\n",
    "diarization = pipeline(f\"{audio_file_folder}{audio_name}{audio_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Iteration über die Diarization-Resultate\n",
    "\n",
    "- `diarization.itertracks(yield_label=True)` liefert für jeden `turn` den Zeitraum (`start`, `end`) und das `speaker`-Label.\n",
    "- Wir sammeln diese in einer Liste (`diarization_data`)\n",
    "\n",
    "So erhält man am Ende ein Array mit Einträgen wie:\n",
    "```json\n",
    "{\n",
    "  \"start\": 0.0,\n",
    "  \"end\": 1.23,\n",
    "  \"speaker\": \"SPEAKER_00\"\n",
    "}\n",
    "```\n",
    "\n",
    "- Mit `json.dumps(diarization_data, indent=4)` können wir das gesamte Dictionary (mit Timestamps und Speaker Labels etc.) schön formatiert ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"start\": 0.03096875,\n",
      "        \"end\": 0.047843750000000004,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 0.30096875,\n",
      "        \"end\": 6.949718750000001,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 7.21971875,\n",
      "        \"end\": 12.754718750000002,\n",
      "        \"speaker\": \"SPEAKER_00\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 13.395968750000002,\n",
      "        \"end\": 13.497218750000002,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 13.497218750000002,\n",
      "        \"end\": 13.56471875,\n",
      "        \"speaker\": \"SPEAKER_02\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 13.56471875,\n",
      "        \"end\": 14.391593750000002,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 14.391593750000002,\n",
      "        \"end\": 15.032843750000001,\n",
      "        \"speaker\": \"SPEAKER_02\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 15.032843750000001,\n",
      "        \"end\": 22.457843750000002,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 22.69409375,\n",
      "        \"end\": 27.43596875,\n",
      "        \"speaker\": \"SPEAKER_00\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 28.02659375,\n",
      "        \"end\": 28.094093750000003,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 28.094093750000003,\n",
      "        \"end\": 28.245968750000003,\n",
      "        \"speaker\": \"SPEAKER_00\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 28.245968750000003,\n",
      "        \"end\": 28.27971875,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 28.27971875,\n",
      "        \"end\": 28.296593750000003,\n",
      "        \"speaker\": \"SPEAKER_00\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 28.296593750000003,\n",
      "        \"end\": 29.02221875,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 29.02221875,\n",
      "        \"end\": 29.32596875,\n",
      "        \"speaker\": \"SPEAKER_00\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 29.32596875,\n",
      "        \"end\": 29.359718750000003,\n",
      "        \"speaker\": \"SPEAKER_01\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Ergebnisse in strukturierter Form sammeln\n",
    "diarization_data = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    diarization_data.append({\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end,\n",
    "        \"speaker\": speaker\n",
    "    })\n",
    "\n",
    "# Ergebnis als JSON ausgeben\n",
    "print(json.dumps(diarization_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Datei Speichern\n",
    "- `diarization_data` enthält das Ergebniss der Diarization (Sprechertrennung).\n",
    "- Nun speichern wir das Ergebnis in einer `.json`-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Datei speichern\n",
    "with open(f\"{pyannote_folder}{audio_name}.json\", \"w\") as json_file:\n",
    "    json.dump(diarization_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Hier wurde gezeigt, wie man eine **Speaker Diarization** mithilfe von `pyannote.audio` durchführt:\n",
    "1. Laden von Modell und Token aus den Umgebungsvariablen (`dotenv`).\n",
    "2. Anlegen einer Pipeline (z. B. `pyannote/speaker-diarization`).\n",
    "3. Verarbeitung der Audiodatei und Iteration über die Zeitschnitte.\n",
    "4. Speichern der Ergebnisse als strukturierte JSON-Datei, um später darauf zugreifen zu können.\n",
    "\n",
    "Nun kannst du die **Sprecherabschnitte** z. B. für Transkript-Annotationen, Audio-Schnitt, oder jede andere Anwendung nutzen, bei der du wissen musst, *wer wann spricht*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
