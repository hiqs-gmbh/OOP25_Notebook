{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzung von Language Models (LLMs) mit Python, LangChain & Co.\n",
    "In diesem Workshop lernst du Schritt für Schritt, wie du Language Models (LLMs) in Python einbindest. Wir nutzen dafür verschiedene Bibliotheken:\n",
    "\n",
    "- `dotenv` zum Laden von Umgebungsvariablen,\n",
    "- `pydantic` zum strukturierten Validieren unserer Ausgaben,\n",
    "- `langchain_openai` für die Integration mit (OpenAI-kompatiblen) Modellen,\n",
    "- `langchain_ollama` für die Integration mit Ollama,\n",
    "- und eine kleine interne Bibliothek `langchain_core.messages` für die Message-Klassen HumanMessage, SystemMessage und AIMessage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain-spezifische Imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Laden der Umgebungsvariablen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Variablen aus .env (falls vorhanden)\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://host.docker.internal:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Konfiguration für Ollama (lokales LLM)\n",
    "\n",
    "Hier erstellen wir eine Instanz des Ollama Chat LLMs, einen sogenannten Wrapper (um ein LLM). \n",
    "\n",
    "> **_Achtung:_**  Bevor wir Ollama nutzen können, müssen wir zunächst Ollama über einen Docker Container bzw. die `docker-compose.yml` starten. Dass müssen wir außerhalb des DevContainers tun, indem wir `docker compose up` im Verzeichnis des Repos in der Konsole eingeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer Instanz des Ollama-Chat-LLMs\n",
    "ollama_llm = ChatOllama(\n",
    "    model=OLLAMA_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erklärung:\n",
    "\n",
    "- `os.getenv(\"OLLAMA_BASE_URL\", \"http://host.docker.internal:11434\")` versucht erst den Wert aus den Umgebungsvariablen zu lesen. Falls nichts gefunden wird, nutzen wir den Default-Wert.\n",
    "- `ChatOllama` ist eine Klasse aus `langchain_ollama`, die uns eine einfache Schnittstelle zum Ollama-Server bietet.\n",
    "- Wir setzen `temperature=0.8`, was bedeutet, dass die Antworten etwas zufälliger (kreativer) ausfallen als mit dem Standardwert (z. B. 0.7). Je höher die Temperatur, desto \"kreativer\" bzw. \"zufälliger\" sind die Antworten.\n",
    "\n",
    "[Hier](https://python.langchain.com/v0.2/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html) kannst du nachschauen, was du alles bei deinem Modell einstellen kannst.\n",
    "\n",
    "[Hier](https://ollama.com/library) kannst du nachschauen, was für Modelle noch so von Ollama unterstützt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Erste Abfrage an dein lokales Ollama-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first explained it in the late 19th century. Here's a simplified explanation:\n",
      "\n",
      "When sunlight enters Earth's atmosphere, it consists of a spectrum of colors, including all the colors of the visible light spectrum (red, orange, yellow, green, blue, indigo, and violet). These colors are separated by different wavelengths, with shorter wavelengths (like violet) being scattered more than longer wavelengths (like red).\n",
      "\n",
      "The scattering of sunlight occurs when tiny molecules of gases in the atmosphere, such as nitrogen (N2) and oxygen (O2), interact with the light. The smaller molecules scatter the shorter wavelengths (blue and violet) more than the longer wavelengths (red and orange), which is known as Rayleigh scattering.\n",
      "\n",
      "As a result, the blue light is scattered throughout the atmosphere, giving the sky its blue appearance. This is especially noticeable during the daytime when the sun is overhead, as the scattered light scatters in all directions, making it appear blue to our eyes.\n",
      "\n",
      "It's worth noting that the color of the sky can change depending on various factors, such as:\n",
      "\n",
      "* Dust and pollution in the atmosphere\n",
      "* Water vapor and other gases in the air\n",
      "* Time of day (during sunrise and sunset, the light has to travel through more of the atmosphere)\n",
      "* Cloud cover\n",
      "\n",
      "But overall, Rayleigh scattering is the main reason why the sky appears blue.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Why is the sky blue?'\n",
    "\n",
    "messages_for_llm = [\n",
    "    # SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=prompt)\n",
    "]\n",
    "\n",
    "answer = ollama_llm.invoke(messages_for_llm)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erklärung:\n",
    "\n",
    "- Wir erstellen ein prompt mit der Frage: `\"Why is the sky blue?\"`\n",
    "- `messages_for_llm` ist eine Liste, die (mindestens) ein `HumanMessage` enthält. (Optional kann man auch ein `SystemMessage` hinzufügen, um den Kontext/Verhalten des Modells zu steuern.)\n",
    "- `invoke(...)` ruft tatsächlich das Large Language Model auf und gibt die Antwort zurück.\n",
    "- Mit `answer.content` greifen wir auf den reinen Text der generierten Antwort zu und geben ihn aus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Nutzung von strukturiertem Output mit Pydantic\n",
    "Das folgende Code-Beispiel zeigt, wie man den Output eines LLM direkt in ein Pydantic-Modell parsen kann. Das ist sehr praktisch, wenn wir z. B. JSON-ähnliche Antworten erwarten oder festes Schema (Felder) haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called scattering, where light from the sun is broken down into its individual colors (red, orange, yellow, green, blue, and violet) by the tiny molecules of gases in the Earth's atmosphere.\n"
     ]
    }
   ],
   "source": [
    "class QuestionAnswer(BaseModel):\n",
    "    \"\"\"A model with an answer field\"\"\"\n",
    "    answer_as_string: str = Field(description=\"The answer generated\")\n",
    "\n",
    "# Vorbereitung unserer Nachrichten\n",
    "messages_for_llm = [\n",
    "    HumanMessage(content=prompt)\n",
    "]\n",
    "\n",
    "# Wir nutzen .with_structured_output(...) um die Antwort direkt zu validieren\n",
    "structured_llm = ollama_llm.with_structured_output(QuestionAnswer)\n",
    "answer_structured = structured_llm.invoke(messages_for_llm)\n",
    "print(answer_structured.answer_as_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erklärung:\n",
    "\n",
    "- `QuestionAnswer` ist ein Pydantic-Datenmodell. Es definiert, wie die Antwort von LLM aussehen soll – in diesem Fall mit dem Feld `answer_as_string`.\n",
    "- `with_structured_output(QuestionAnswer)` instruiert LangChain, dass die Antwort vom LLM dem Schema `QuestionAnswer` entsprechen soll (intern wird das Prompting entsprechend angepasst und versucht, die Antwort im passenden JSON-Format zurückzubekommen).\n",
    "- `answer_structured.answer_as_string` liefert schließlich den generierten Text zurück."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Alternative: StackIT LLM mit OpenAI-kompatiblem Endpoint\n",
    "Nun wollen wir noch ein anderes LLM ansprechen – z. B. über **StackIT**. Dieses funktioniert mit einem OpenAI-kompatiblen Endpoint und kann daher über ChatOpenAI (aus langchain_openai) verwendet werden.\n",
    "\n",
    "Dazu müssen wir zunächst den API_KEY in die `.env` oder in der Code Zeile darunter einsetzen. \n",
    "\n",
    "Der Vorteil dieses Modells ist, dass es im Gegensatz zu unserem lokalen LLM viel mehr Parameter hat (70 Milliarden) und damit bessere Ergebnisse liefert. Dafür läuft es auf 4 L40S Nvidia GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_as_string='The sky appears blue because of a phenomenon called Rayleigh scattering, which is the scattering of sunlight by small particles or molecules in the atmosphere. The blue color we see in the sky is due to the scattering of shorter (blue) wavelengths of light by the tiny molecules of gases such as nitrogen and oxygen, while the longer (red) wavelengths pass straight through. This scattering effect gives the sky its blue appearance during the daytime.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Wir lesen die nötigen Umgebungsvariablen ein\n",
    "STACKIT_VLLM_API_KEY = os.getenv(\"STACKIT_VLLM_API_KEY\", \"\")\n",
    "STACKIT_MODEL = os.getenv(\"STACKIT_MODEL\", \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\")\n",
    "STACKIT_BASE_URL = os.getenv(\"STACKIT_BASE_URL\", \"https://api.openai-compat.model-serving.eu01.onstackit.cloud/v1\")\n",
    "\n",
    "# Erstellen der ChatOpenAI-Instanz\n",
    "stackit_llm = ChatOpenAI(\n",
    "    model=STACKIT_MODEL,\n",
    "    base_url=STACKIT_BASE_URL,\n",
    "    api_key=STACKIT_VLLM_API_KEY,\n",
    ")\n",
    "\n",
    "prompt = \"Why is the sky blue?\"\n",
    "\n",
    "messages_for_stackit = [HumanMessage(content=prompt)]\n",
    "structured_llm_stackit = stackit_llm.with_structured_output(QuestionAnswer)\n",
    "answer_stackit = structured_llm_stackit.invoke(messages_for_stackit)\n",
    "\n",
    "print(answer_stackit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erklärung:\n",
    "\n",
    "- `ChatOpenAI(...)` nimmt ähnliche Parameter wie die offiziellen OpenAI-Clients. Wir müssen lediglich base_url und api_key anpassen, um den Service von StackIT anzusprechen.\n",
    "- Auch hier nutzen wir wieder `with_structured_output(QuestionAnswer)`, um das Antwortformat festzulegen.\n",
    "- Mit `print(answer_stackit)` bekommen wir die Daten entsprechend unserem Pydantic-Modell zurück (in diesem Fall enthält das Objekt ein Feld answer_as_string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "In diesem Workshop hast du gelernt:\n",
    "\n",
    "1. Wie man Umgebungsvariablen einsetzt, um sensible Daten (z. B. API-Keys, Endpunkte) zu trennen und flexibel zu halten.\n",
    "2. Wie man mit LangChain unterschiedliche LLMs anspricht, sowohl Ollama als auch OpenAI-kompatible Modelle.\n",
    "3. Wie man mittels Pydantic und der LangChain-Methode with_structured_output() Antworten von LLMs direkt in ein festes Schema gießen kann, um sie leicht weiterzuverarbeiten.\n",
    "\n",
    "Die gezeigten Codebeispiele sollten dich in die Lage versetzen, eigene LLM-basierte Anwendungen oder Chatbots zu entwickeln. Die wichtigsten Prinzipien sind dabei:\n",
    "- Prompting (welche Eingaben/Anweisungen gebe ich an das Modell?),\n",
    "- Nachrichtenformat (System-/Human-/AI-Nachrichten in langchain_core.messages),\n",
    "- Strukturierte Validierung (z. B. mit Pydantic).\n",
    "\n",
    "Viel Erfolg bei deinen Experimenten mit Language Models!\n",
    "\n",
    "### Weiterführende Ideen oder Challenge:\n",
    "\n",
    "- Erweitere das Pydantic Modell um weitere Ausgabefelder (bspw. eine Liste) und versuche durch die Validierung des Pydantic Parsers zu \"bestehen\".  \n",
    "- Binde das System in ein FastAPI-Backend ein und nutze die strukturierten Antworten, um JSON-Antworten an einen Frontend-Client zu liefern.\n",
    "- Experimentiere mit unterschiedlichen Temperature-Einstellungen und Prompting-Techniken (z. B. few-shot, chain-of-thought).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
