{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenführen von Whisper-Transkript und Pyannote-Diarization (Matching)\n",
    "\n",
    "In diesem Workshop lernst du, wie man das **transkribierte** Ergebnis von **Whisper** (Chunks mit Zeitstempeln und Text) mit dem Ergebnis der **Pyannote Speaker Diarization** (Sprecher und deren Zeitintervalle) kombiniert. Ziel ist es, jedem Text-Chunk einen Sprecher zuzuweisen.\n",
    "\n",
    "Voraussetzungen:\n",
    "- Eine Datei mit dem **Whisper**-Output, z. B. `whisper_data.json`, die zumindest folgende Struktur hat:\n",
    "  ```json\n",
    "  {\n",
    "    \"text\": \"...\",\n",
    "    \"chunks\": [\n",
    "      {\n",
    "        \"timestamp\": [ start, end ],\n",
    "        \"text\": \"...\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- Eine Datei mit den Pyannote-Ergebnissen, z. B. diarization_data.json, welche ein Array von Segmenten enthält:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"start\": 0.0,\n",
    "    \"end\": 1.5,\n",
    "    \"speaker\": \"SPEAKER_00\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dateipfade spezifizieren\n",
    "- `whisper_folder` verweist auf den Ordner mit den Whisper-Ergebnissen.\n",
    "- `pyannote_folder` verweist auf den Ordner mit den Pyannote-Ergebnissen.\n",
    "- `matched_transcripts_folder` ist der Zielordner, in dem wir die **kombinierten** Daten ablegen.\n",
    "- `audio_file_name` ist der Basename der entsprechenden Audiodatei (z. B. `\"Sonic-Manus-Pitch\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spezifierung von Verzeichnissen und Dateinamen\n",
    "whisper_folder = \"./outputs/whisper/\"\n",
    "pyannote_folder = \"./outputs/pyannote/\"\n",
    "matched_transcripts_folder = \"./outputs/matched_transcripts/\"\n",
    "audio_file_name = \"Michi_Sonic_Manus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Einlesen der JSON-Dateien\n",
    "Wir lesen:\n",
    "1. Die Whisper-Ergebnisse (`whisper_data.json`) mit den Chunks.\n",
    "2. Die Pyannote-Ergebnisse (`diarization_data.json`) mit den Sprechersegmenten.\n",
    "\n",
    "Anschließend liegen uns zwei Python-Objekte vor:\n",
    "- `whisper_data` mit u. a. `whisper_data[\"chunks\"]`\n",
    "- `diarization_data` als Liste von Dictionaries mit `[{\"start\":..., \"end\":..., \"speaker\":...}, ...]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dateien einlesen\n",
    "with open(f\"{whisper_folder}{audio_file_name}.json\", \"r\") as whisper_file:\n",
    "    whisper_data = json.load(whisper_file)\n",
    "\n",
    "with open(f\"{pyannote_folder}{audio_file_name}.json\", \"r\") as diarization_file:\n",
    "    diarization_data = json.load(diarization_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Zusammenführen der Chunks\n",
    "\n",
    "Für jeden **Whisper-Chunk** (mit `start`, `end` und `text`) suchen wir den passenden **Sprecher** in den Diarization-Daten. \n",
    "\n",
    "**Logik**:\n",
    "- Falls das Zeitintervall `[chunk_start, chunk_end]` vollständig in `[segment.start, segment.end]` liegt, ordnen wir diesen Sprecher zu.\n",
    "- Falls es eine Überlappung gibt (nicht 100%ig passend, aber Start oder Ende überschneiden sich), weisen wir auch diesen Sprecher zu. \n",
    "\n",
    "Wenn kein Sprecher-Segment gefunden wird, setzen wir `speaker = \"Unknown\"` und geben eine Warnung aus.\n",
    "\n",
    "\n",
    "Am Ende erstellen wir ein Array `merged_output`, in dem jeder Eintrag so aufgebaut ist:\n",
    "```json\n",
    "{\n",
    "  \"speaker\": \"SPEAKER_XX\",\n",
    "  \"text\": \"...\",\n",
    "  \"start\": 0.0,\n",
    "  \"end\": 1.2\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste für die zusammengeführten Ergebnisse\n",
    "merged_output = []\n",
    "\n",
    "# Durch alle Whisper-Chunks iterieren\n",
    "for chunk in whisper_data[\"chunks\"]:\n",
    "    chunk_start = chunk[\"timestamp\"][0]\n",
    "    chunk_end = chunk[\"timestamp\"][1]\n",
    "    chunk_text = chunk[\"text\"]\n",
    "\n",
    "    # Standard-Sprecher = \"Unknown\"\n",
    "    speaker = \"Unknown\"\n",
    "\n",
    "    # Überprüfe, ob dieses Chunk mit einem Diarization-Segment übereinstimmt\n",
    "    for segment in diarization_data:\n",
    "        if chunk_start >= segment[\"start\"] and chunk_end <= segment[\"end\"]:\n",
    "            # Das Whisper-Chunk liegt komplett in diesem Segment\n",
    "            speaker = segment[\"speaker\"]\n",
    "            break\n",
    "        elif chunk_start <= segment[\"end\"] and chunk_end >= segment[\"start\"]:\n",
    "            # Teil-Überlappung\n",
    "            speaker = segment[\"speaker\"]\n",
    "            break\n",
    "\n",
    "    if speaker == \"Unknown\":\n",
    "        print(f\"Warning: No speaker found for chunk '{chunk_text}' [{chunk_start:.2f}s to {chunk_end:.2f}s]\")\n",
    "\n",
    "    merged_output.append({\n",
    "        \"speaker\": speaker,\n",
    "        \"text\": chunk_text.strip(),\n",
    "        \"start\": chunk_start,\n",
    "        \"end\": chunk_end\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ausgabe von Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"H\\u00e4tte ich noch nicht auf der HHTS Webseite und habe da dem Blog bei Track zu Sonic Manus gesehen\",\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 6.56\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"und es hat mich sehr neugierig gemacht.\",\n",
      "        \"start\": 6.56,\n",
      "        \"end\": 8.36\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"Kannst du das f\\u00fcr meine Zuschauer kurz eine 2-3 Setze beschreiben?\",\n",
      "        \"start\": 8.36,\n",
      "        \"end\": 12.16\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Ja klar kann ich.\",\n",
      "        \"start\": 12.16,\n",
      "        \"end\": 13.64\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Sonic Manus kannst du ganz einfach Meetings erstellen und dann Audi Datein hochladen und die\",\n",
      "        \"start\": 13.64,\n",
      "        \"end\": 19.0\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"werden dann transkippiert.\",\n",
      "        \"start\": 19.0,\n",
      "        \"end\": 20.0\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Was genau meinst du mit transkippiert?\",\n",
      "        \"start\": 20.0,\n",
      "        \"end\": 22.64\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Na ja da wird dann festgelegt, wer was gesagt hat und was gesagt wurde.\",\n",
      "        \"start\": 22.64,\n",
      "        \"end\": 27.28\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"Das hei\\u00dft, wenn mir jetzt hier soll ich mal uns laufen lassen w\\u00fcrden, dann h\\u00e4tt ich danach\",\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 6.0\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"ne Datei oder ne, also die Audio Datei von dem, was wir gesagt haben und getrennt\",\n",
      "        \"start\": 6.0,\n",
      "        \"end\": 12.32\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"bei was gesagt hat.\",\n",
      "        \"start\": 12.32,\n",
      "        \"end\": 13.32\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Wichtig.\",\n",
      "        \"start\": 13.32,\n",
      "        \"end\": 14.32\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Und dann als Audio oder wie werden die Daten bei der Frau beidet?\",\n",
      "        \"start\": 14.32,\n",
      "        \"end\": 17.32\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Ja richtig.\",\n",
      "        \"start\": 17.32,\n",
      "        \"end\": 18.32\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Wir haben dann das gesprochen, ne Wort und aufgeteilt eben, welcher Sprecher was gesagt\",\n",
      "        \"start\": 18.32,\n",
      "        \"end\": 23.44\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"hat.\",\n",
      "        \"start\": 23.44,\n",
      "        \"end\": 24.44\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_01\",\n",
      "        \"text\": \"Ich kann dann auch dein Name zum Beispiel eintragen und ist tolle dann ist, der wird\",\n",
      "        \"start\": 24.44,\n",
      "        \"end\": 27.32\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"automatisch noch eine Zusammenfassung generiert und in der Zusammenfassung sind verschiedene Stichpunkte\",\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 4.8\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"SPEAKER_00\",\n",
      "        \"text\": \"enthalten welche Themen da abgesprochen wurden genau cool\",\n",
      "        \"start\": 4.8,\n",
      "        \"end\": 9.72\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(merged_output, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Datei Speichern\n",
    "\n",
    "- Nun speichern wir das Ergebnis in einer `.json` in den Zielordner (`matched_transcripts_folder`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Datei speichern\n",
    "with open(f\"{matched_transcripts_folder}{audio_file_name}.json\", \"w\") as output_file:\n",
    "    json.dump(merged_output, output_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Schritt haben wir:\n",
    "1. **Whisper**-Transkripte (Text + Zeitstempel) eingelesen.\n",
    "2. **Pyannote**-Diarization-Segmente (Sprecher + Zeitintervalle) eingelesen.\n",
    "3. Für jedes Whisper-Chunk basierend auf dem Zeitintervall einen **Sprecher** ermittelt.\n",
    "4. Das Ergebnis in einer JSON-Datei gespeichert.\n",
    "\n",
    "Damit hast du eine **Sprecher-annotierte Transkription**, in der jeder Abschnitt weiß, von wem er gesprochen wurde. Diese Daten können z. B. für **Interviews**, **Meetings** oder **Podcasts** sehr hilfreich sein, um mehr Informationen als nur Text zu haben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
