{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text mit Hugging Face Transformers (Whisper)\n",
    "\n",
    "In diesem Workshop lernst du, wie du mithilfe von **Hugging Face**-Modellen (speziell Whisper) Sprachaufnahmen automatisiert transkribieren kannst. Wir verwenden dabei das Modell [openai/whisper-base](https://huggingface.co/openai/whisper-base).\n",
    "\n",
    "Voraussetzungen:\n",
    "- Installierte Pakete:\n",
    "  - `transformers`\n",
    "  - `torchaudio`\n",
    "  - `tqdm`\n",
    "  - `huggingface_hub`\n",
    "- Eine Audio-Datei (z. B. `.mp3`, `.wav`) zum Testen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dateipfade spezifizieren\n",
    "- Wir definieren Ordner und Dateinamen (z. B. `audio_file_folder`, `audio_name`, `audio_format`, `whisper_folder`).\n",
    "- Lege in `audio_file_folder` deine eigenen Audiodateien ab und ändere `audio_name` entsprechend, um deine eigenen Dateien zu transkribieren\n",
    "- Damit wir die Ausgabe von Whisper nachher weiterverwenden können, speichern wir die Ausgabe später in einer .json File im `whisper_folder` ab. \n",
    "\n",
    "Wir benötigen eine **mp3** Datei als Eingabe für Whisper. Mit **ffmpeg** können wir Audiodateien von anderem Format konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: audio_name: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i ./audios/<audio_name>.m4a ./audios/<audio_name>.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spezifierung von Verzeichnissen und Dateinamen\n",
    "audio_file_folder = \"./audios/\"\n",
    "audio_name = \"Sonic-Manus-Pitch\"\n",
    "audio_format = \".mp3\"\n",
    "whisper_folder = \"./outputs/whisper/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Device wählen (GPU oder CPU)\n",
    "Wir wählen automatisch die GPU aus, wenn `torch.cuda.is_available()` den Wert `True` zurückgibt. Ansonsten fallen wir auf die CPU zurück. Zudem passen wir den **Datentyp** (`torch_dtype`) an, um auf der GPU mit `float16` rechnen zu können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Laden des Modells und Prozessors\n",
    "\n",
    "- **AutoModelForSpeechSeq2Seq**: Lädt ein Modell, das speziell für die Sprach-zu-Text-Aufgaben (Speech-to-Text) ausgelegt ist.\n",
    "- **AutoProcessor**: Lädt den passenden Tokenizer und Audio-Feature-Extractor, um die Audiodatei in das richtige Input-Format für das Modell zu konvertieren.\n",
    "\n",
    "Anschließend verschieben wir das Modell auf das gewählte `device`.\n",
    "\n",
    "Das [Modell wird von Huggingface](https://huggingface.co/openai/whisper-large-v3-turbo) heruntergeladen. Dort findest du auch weitere interessante Informationen über das Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell-ID von Whisper\n",
    "# model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model_id = \"openai/whisper-base\"\n",
    "\n",
    "# Modell laden\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Prozessor laden\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Festlegen der Sprache\n",
    "\n",
    "Mit `forced_decoder_ids` legen wir fest, dass das Modell **Deutsch** transkribieren soll. \n",
    "Der Aufruf `processor.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")` erzeugt die IDs, die sicherstellen, dass das Modell deutschsprachigen Text als Output generiert.\n",
    "\n",
    "Probiere gerne andere Sprachen aus und schaue wie das funktioniert passiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forced Decoder IDs für Deutsch (Transkription)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline erstellen\n",
    "\n",
    "Wir erstellen eine Pipeline vom Typ `\"automatic-speech-recognition\"`. Damit:\n",
    "- wird das Modell und der Tokenizer verknüpft\n",
    "- das Audio-Feature-Extrahieren übernimmt `feature_extractor`\n",
    "- im `generate_kwargs` können wir die `forced_decoder_ids` übergeben\n",
    "- mit `return_timestamps=True` erhalten wir ggf. Wort- oder Chunk-basierte Zeitstempel\n",
    "\n",
    "So können wir direkt eine Audiodatei an `pipe(...)` übergeben, ohne manuell Tokenizing etc. durchzuführen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline einrichten\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Transkribieren der Datei\n",
    "- Das Modell transrkribiert die Datei\n",
    "- `pipe(...)` gibt ein Dictionary mit Feldern zurück (z. B. `\"text\"` für die transkribierte Ausgabe, `\"chunks\"` für die Zeitstempel etc.).\n",
    "- Mit `json.dumps(result, indent=4)` können wir das gesamte Dictionary (mit Timestamps, Token-Infos etc.) schön formatiert ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"text\": \" Moin, was ist denn das Sonic Manus Projekt? Das Sonic Manus Projekt ist ein kairosiertes Transcription Tool. Das Tolle daran ist, dass es auch die unterschiedlichen Sprecher in einem Gespr\\u00e4ch aufgeteilt bekommt. Okay, aber das kann Teams doch l\\u00e4ngst. Er schon, aber Teams k\\u00f6nnen daf\\u00fcr einfach die unterschiedlichen Ger\\u00e4te von den Benutzern verwenden und wir k\\u00f6nnen das ganz von alleine. Genau, wir brauchen nur das Audio und zus\\u00e4tzlich l\\u00e4uft es bei uns lokal. Mika will ich haben.\",\n",
      "    \"chunks\": [\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                0.0,\n",
      "                3.2\n",
      "            ],\n",
      "            \"text\": \" Moin, was ist denn das Sonic Manus Projekt?\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                3.2,\n",
      "                7.04\n",
      "            ],\n",
      "            \"text\": \" Das Sonic Manus Projekt ist ein kairosiertes Transcription Tool.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                7.04,\n",
      "                13.0\n",
      "            ],\n",
      "            \"text\": \" Das Tolle daran ist, dass es auch die unterschiedlichen Sprecher in einem Gespr\\u00e4ch aufgeteilt bekommt.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                13.0,\n",
      "                15.88\n",
      "            ],\n",
      "            \"text\": \" Okay, aber das kann Teams doch l\\u00e4ngst.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                15.88,\n",
      "                21.0\n",
      "            ],\n",
      "            \"text\": \" Er schon, aber Teams k\\u00f6nnen daf\\u00fcr einfach die unterschiedlichen Ger\\u00e4te von den Benutzern verwenden\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                21.0,\n",
      "                22.6\n",
      "            ],\n",
      "            \"text\": \" und wir k\\u00f6nnen das ganz von alleine.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                22.6,\n",
      "                27.6\n",
      "            ],\n",
      "            \"text\": \" Genau, wir brauchen nur das Audio und zus\\u00e4tzlich l\\u00e4uft es bei uns lokal.\"\n",
      "        },\n",
      "        {\n",
      "            \"timestamp\": [\n",
      "                0.0,\n",
      "                2.0\n",
      "            ],\n",
      "            \"text\": \" Mika will ich haben.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = pipe(f\"{audio_file_folder}{audio_name}{audio_format}\", generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids})\n",
    "\n",
    "# Ergebnis als JSON ausgeben\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Datei Speichern\n",
    "- `result[\"text\"]` enthält den transkribierten Text.\n",
    "- Nun speichern wir das Ergebnis in einer `.json`-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Datei speichern\n",
    "with open(f\"{whisper_folder}{audio_name}.json\", \"w\") as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Workshop haben wir:\n",
    "1. Ein **Whisper-Modell** (openai/whisper-large-v3-turbo) geladen und konfiguriert.\n",
    "2. Eine **Pipeline** für die automatische Spracherkennung eingerichtet.\n",
    "3. **Forced Decoder IDs** benutzt, um Deutsch als Transkriptionssprache festzulegen.\n",
    "4. Eine **Audiodatei** transkribiert und das Ergebnis als Text und JSON ausgegeben.\n",
    "\n",
    "Damit hast du ein Grundgerüst, um eigene Audiodateien zu transkribieren oder Übersetzungen in andere Sprachen durchzuführen. \n",
    "- Passe je nach Bedarf die `forced_decoder_ids` an, um unterschiedliche Sprachen zu erzwingen.\n",
    "- Experimentiere mit verschiedenen Whisper- oder anderen Speech-to-Text-Modellen von Hugging Face.\n",
    "- Nutze die Timestamps für die Anzeige von Untertiteln oder Live-Transkriptionen.\n",
    "\n",
    "Viel Erfolg!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
